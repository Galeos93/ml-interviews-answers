> Given a dataset of credit card purchases information, each record is labelled as fraudulent or safe, how would you build a fraud detection algorithm?

(To the interviewer) What latency or throughput contstrains have the model? Where will it be served, on an edge device, or on a server? How is the structure of the data, is it a temporal series for different users that contains the different purchases?

(Possible interviewer's answer) We would be looking for something with low latency that would be deployed on a cloud instance. As you said, we would have a temporal series with the purchases.

Given the dataset, I would start with an Exploratory Data Analysis. With this analysis I would try to find some patterns on the data that could help me infer the class. On a Machine Learning problem, knowing your data is the most important thing. After I have studied the data, I would note any possible problems with it, such as the existence of missing data. There are various ways to deal with this, such as removing the samples with missing data, impute the data based on the rest, etc. Before any of this is done, it is key we understand why some data is missing. If most of the fraudulent samples happen to have missing data and we decide to remove those samples, we would be in trouble. Before we with strategies such as data imputation, first we must split our data. Preferably into train, validation and tests sets. Again, we must be careful about this. Sometimes, splitting data incorrectly may incur into the data leakage phenomenon. For this case, random shuffling and a posterior split would be enough. Once we have our split ready, we can proceed to preprocess the data with the information we have gathered on the EDA, such as the missing data.

Now that we have the data ready, we can proceed to choose what model to use. We have the labels, so an appropiate choice would be a supervised learning algorithm. Given that we are dealing with a temporal series, an algorithm that can find patterns in a data sequence would be suitable. Something variation of RNN could be good, such as LSTMs (Long-Short Term Memory) or GRUs (Gated Recurrent Units). GRUs may be better suited, since these models have less parameters and we the model to have low latency. We would set the training environment ready on an script, so we can lunch multiple experiments. Regarding the features, we could feed the temporal series directly, being careful about the different ranges that the data may have. For example, if in our purchases information we have "money" and "quantity", it is very likely the "money" feature has a bigger scale, so we would need to normalize it.

After all this experimentation, we could save the best model and preferably some "Model Card" that explains how it was trained and other useful metadata.